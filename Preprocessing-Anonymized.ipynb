{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, all necessary libraries will be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys\n",
    "import csv\n",
    "from collections import Counter\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "my_stop = stopwords.words('dutch')\n",
    "stemmer = SnowballStemmer('dutch')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "# STTM lib from Github\n",
    "from gsdmm import MovieGroupProcess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening all text files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening our previous created dataframe. We do not have consent to share the data including the Text messages. For this reasons the analysis steps have been muted, but the taken steps can be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buzz_df2 = pd.read_csv(\"Input/Text/buzz5.csv\", engine = \"python\")\n",
    "#buzz_df2[\"Sender\"] = buzz_df2[\"Sender\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all empty text were removed from the text datafiles. They were  marked as NA and subsequently deleted.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buzz_df2['Text'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how much data is left, once empty Texts are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buzz_df2 = buzz_df2[buzz_df2['Text'].notna()]\n",
    "#print(len(buzz_df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a lot of duplicate texts (due to re-sharing of messages). In the current analysis we were only interested in unique text, only only unique messages were retained. This considerably narrows the dataset down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_buzz = buzz_df2.drop_duplicates(subset=['Text'])\n",
    "\n",
    "#print(len(unique_buzz2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the preprocessing: All text data will be tokenized, so that we split single messages into single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_buzz['tokens'] = unique_buzz['Text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further cleaning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. If a character appears three times+ in a row (heeeeeey) reduce it to two\n",
    "#unique_buzz['triples'] = unique_buzz['tokens'].apply(lambda x: [re.sub(r'(\\w)\\1+',r'\\1\\1', w) for w in x ])\n",
    "\n",
    "# 2. Lowercase the text                                                              \n",
    "#unique_buzz['low'] = unique_buzz['triples'].map(lambda x: list(map(str.lower, x)))\n",
    "\n",
    "# 3. Remove Dutch stop-words\n",
    "#unique_buzz['stop'] = unique_buzz['low'].apply(lambda x: [item for item in x if item not in my_stop])\n",
    "\n",
    "# 4. Remove one-letter fragments\n",
    "#unique_buzz['stop'] = unique_buzz['low'].map(lambda x: [y for y in x if len(y) > 1])\n",
    "\n",
    "# 5. Remove punctuation\n",
    "#unique_buzz['punctuation'] = unique_buzz['stop'].apply(lambda x: [re.sub(\"\\'\", \"\", w) for w in x])\n",
    "\n",
    "# 6. Stemming of the single tokens\n",
    "#unique_buzz['stem'] = unique_buzz['punctuation'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "# Convert the stems to a single string (later necessary for some analysis)\n",
    "#unique_buzz[\"stem_string\"] = [','.join(map(str, l)) for l in unique_buzz['stem']]\n",
    "\n",
    "# Only keep Text which are send by the kids and not the ones which are sent by the mymovez team\n",
    "#unique_buzz = unique_buzz[unique_buzz[\"Type\"] != \"mymovez\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding food in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we aim to filter out communication which is related to food and eating. For this first scroll to the end of the document and run the food-dictionaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Input/Dictionaries/eten_en_drinken_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    eten_en_drinken3 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/foods_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    foods2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/drinks_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    drinks2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/diet_related_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    diet_related2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/supermarkets_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    supermarkets2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/internet_slang_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    internet_slang2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/restaurants_cafes_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    restaurants_cafes2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/meals_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    meals2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/dutch_meals_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    dutch_meals2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/food_brands_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    food_brands2 = json.load(fp)\n",
    "    \n",
    "with open(\"Input/Dictionaries/diets_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    diets2 = json.load(fp)\n",
    "\n",
    "with open(\"Input/Dictionaries/embeddings_stem.json\", \"r\", encoding = \"utf-8\", errors = \"ignore\") as fp:\n",
    "    embeddings2 = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we aimed to filter out some words with a double meaning in Dutch, that can refer to a food, but oftentimes do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = open('Input/Dictionaries/double_words.txt', \"r\")\n",
    "double_words = fi.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all word with possible double-meaning from the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublemeaning(item):\n",
    "    for words in item:\n",
    "        if words in double_words:\n",
    "            item.remove(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublemeaning(eten_en_drinken3)\n",
    "doublemeaning(foods2)\n",
    "doublemeaning(drinks2)\n",
    "doublemeaning(diet_related2)\n",
    "doublemeaning(supermarkets2)\n",
    "doublemeaning(internet_slang2)\n",
    "doublemeaning(restaurants_cafes2)\n",
    "doublemeaning(internet_slang2)\n",
    "doublemeaning(diets2)\n",
    "doublemeaning(meals2)\n",
    "doublemeaning(dutch_meals2)\n",
    "doublemeaning(food_brands2)\n",
    "doublemeaning(embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our dictionaries we build masks for each of them, to filter the matching conversations in our dataset which contain at least one word from one of the diet-related dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask3 = unique_buzz.stem.apply(lambda x: any(item for item in eten_en_drinken3 if item in x))\n",
    "#mask4 = unique_buzz.stem.apply(lambda x: any(item for item in embeddings2 if item in x))\n",
    "\n",
    "#mask_food = unique_buzz.stem.apply(lambda x: any(item for item in foods2 if item in x))\n",
    "#mask_drinks = unique_buzz.stem.apply(lambda x: any(item for item in drinks2 if item in x))\n",
    "#mask_restaurants = unique_buzz.stem.apply(lambda x: any(item for item in restaurants_cafes2 if item in x))\n",
    "#mask_supermarkets = unique_buzz.stem.apply(lambda x: any(item for item in supermarkets2 if item in x))\n",
    "#mask_related = unique_buzz.stem.apply(lambda x: any(item for item in diet_related2 if item in x))\n",
    "#mask_slang = unique_buzz.stem.apply(lambda x: any(item for item in internet_slang2 if item in x))\n",
    "#mask_brands = unique_buzz.stem.apply(lambda x: any(item for item in food_brands2 if item in x))\n",
    "#mask_diet = unique_buzz.stem.apply(lambda x: any(item for item in diets2 if item in x))\n",
    "#mask_meals = unique_buzz.stem.apply(lambda x: any(item for item in meals2 if item in x))\n",
    "#mask_Dmeals = unique_buzz.stem.apply(lambda x: any(item for item in dutch_meals2 if item in x))\n",
    "\n",
    "\n",
    "#eat_buzz = unique_buzz[mask_food]\n",
    "#drink_buzz = unique_buzz[mask_drinks]\n",
    "#restaurant_buzz = unique_buzz[mask_restaurants]\n",
    "#supermarket_buzz = unique_buzz[mask_supermarkets]\n",
    "#related_buzz = unique_buzz[mask_related]\n",
    "#slang_buzz = unique_buzz[mask_slang]\n",
    "#brand_buzz = unique_buzz[mask_brands]\n",
    "#diet_buzz = unique_buzz[mask_diet]\n",
    "#meal_buzz = unique_buzz[mask_meals]\n",
    "#dmeal_buzz = unique_buzz[mask_Dmeals]\n",
    "#total_buzz = unique_buzz[mask3]\n",
    "\n",
    "#lets filter and count the occuring words for each category\n",
    "#total_buzz[\"foods\"] = unique_buzz.stem.apply(lambda x: [item for item in foods2 if item in x])\n",
    "#total_buzz[\"drinks\"] = unique_buzz.stem.apply(lambda x: [item for item in drinks2 if item in x])\n",
    "#total_buzz[\"restaurants\"] = unique_buzz.stem.apply(lambda x: [item for item in restaurants_cafes2 if item in x])\n",
    "#total_buzz[\"supermarkets\"] = unique_buzz.stem.apply(lambda x: [item for item in supermarkets2 if item in x])\n",
    "#total_buzz[\"related\"] = unique_buzz.stem.apply(lambda x: [item for item in diet_related2 if item in x])\n",
    "#total_buzz[\"slang\"] = unique_buzz.stem.apply(lambda x: [item for item in internet_slang2 if item in x])\n",
    "#total_buzz[\"brands\"] = unique_buzz.stem.apply(lambda x: [item for item in food_brands2 if item in x])\n",
    "#total_buzz[\"diet\"] = unique_buzz.stem.apply(lambda x: [item for item in diets2 if item in x])\n",
    "##total_buzz[\"meals\"] = unique_buzz.stem.apply(lambda x: [item for item in meals2 if item in x])\n",
    "#total_buzz[\"Dmeals\"] = unique_buzz.stem.apply(lambda x: [item for item in dutch_meals2 if item in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recode the media variable in the following way: 0 = Message contained no attached picture, 1 = message contained a non-diet-related picture, 2 = message contained a diet-related picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_buzz.Media[total_buzz.Media.str.startswith('Foodzz', na=False)] = 2\n",
    "#total_buzz.Media = total_buzz.Media.replace(to_replace =('^[a-zA-Z]+'), value = 1, regex = True)\n",
    "#total_buzz.Media = total_buzz.Media.replace(to_replace =('^[0-9]+\\.'), value = 1, regex = True)\n",
    "\n",
    "#total_buzz[\"Media\"] = total_buzz.Media.fillna(0)\n",
    "#NO Media: 0, Some media: 1, Food media: 2\n",
    "\n",
    "#total_buzz[\"Media\"] = total_buzz[\"Media\"].apply(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create dummy variables for the Media variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_buzz = pd.concat([total_buzz, pd.get_dummies(total_buzz['Media'], prefix='Media')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the number of messages for each dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food items:  635\n",
      "Drink items:  132\n",
      "Restaurant items:  20\n",
      "Supermarket items:  22\n",
      "Diet-related items:  1274\n",
      "Food slang items:  35\n",
      "Food brand items:  26\n",
      "Diet related items:  45\n",
      "Meal items:  194\n",
      "Dutch Meal items:  154\n"
     ]
    }
   ],
   "source": [
    "#print(\"Food items: \", len(unique_buzz[mask_food]))\n",
    "#print(\"Drink items: \",len(unique_buzz[mask_drinks]))\n",
    "#print(\"Restaurant items: \",len(unique_buzz[mask_restaurants]))\n",
    "#print(\"Supermarket items: \",len(unique_buzz[mask_supermarkets]))\n",
    "#print(\"Diet-related items: \",len(unique_buzz[mask_related]))\n",
    "#print(\"Food slang items: \",len(unique_buzz[mask_slang]))\n",
    "#print(\"Food brand items: \",len(unique_buzz[mask_brands]))\n",
    "#print(\"Diet related items: \",len(unique_buzz[mask_diet]))\n",
    "#print(\"Meal items: \",len(unique_buzz[mask_meals]))\n",
    "#print(\"Dutch Meal items: \",len(unique_buzz[mask_Dmeals]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting insight into how the division is between different diet-related categories in the data, we will put them all together again. Subsequently we receive a dataframe with messages, that contain at least one diet-related word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_buzz = total_buzz[mask3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! the food_buzz dataframe now contains all messages which in contain at least one word related to food/eating/drinking etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are left with 2138 sentences, that are about food, eating or drinking\n"
     ]
    }
   ],
   "source": [
    "print(\"We are left with\", len(food_buzz), \"sentences, that are about food, eating or drinking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets filter out the exact food_words in each message and append them to a seperate column in the datadrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_words(text):\n",
    "    #match = []\n",
    "    #for word in eten_en_drinken3:\n",
    "        #if re.search(r'\\b' + word + r'\\b', text):\n",
    "            #match.append(re.findall(r'\\b' + word + r'\\b', text))\n",
    "                        \n",
    "        #else:\n",
    "           # continue\n",
    "    #return match\n",
    "\n",
    "#food_buzz['words'] = food_buzz['stem_string'].apply(get_words)\n",
    "\n",
    "\n",
    "# making a simple list out of list of lists\n",
    "#food_buzz[\"words\"] = food_buzz[\"words\"].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "#counting items\n",
    "#food_buzz['words_no'] = food_buzz['words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_buzz[\"word_string\"] = [','.join(map(str, l)) for l in food_buzz['words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Lets save the results into single dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_buzz.to_pickle('Input/Text/food_buzz.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eat_buzz.to_pickle('Input/Text/eat_buzz.pkl')\n",
    "#drink_buzz.to_pickle('Input/Text/drink_buzz.pkl')\n",
    "#restaurant_buzz.to_pickle('Input/Text/restaurant_buzz.pkl')\n",
    "#supermarket_buzz.to_pickle('Input/Text/supermarket_buzz.pkl')\n",
    "#related_buzz.to_pickle('Input/Text/related_buzz.pkl')\n",
    "#slang_buzz.to_pickle('Input/Text/slang_buzz.pkl')\n",
    "#brand_buzz.to_pickle('Input/Text/brand_buzz.pkl')\n",
    "#diet_buzz.to_pickle('Input/Text/diet_buzz.pkl')\n",
    "#meal_buzz.to_pickle('Input/Text/meal_buzz.pkl')\n",
    "#dmeal_buzz.to_pickle('Input/Text/dmeal_buzz.pkl')\n",
    "#unique_buzz.to_pickle('Input/Text/unique_buzz.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
